{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8DkUEIRdAh8"
      },
      "source": [
        "## Path GAN Discriminator Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xsxwyx7_sqYo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/researcher/anaconda3/envs/control/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RXjk82ZWso7E"
      },
      "outputs": [],
      "source": [
        "class PathGAN_D(nn.Module):\n",
        "    def __init__(self,\n",
        "                 reduced, in_vecs = 32, lstm_actv='tanh'):\n",
        "        super(PathGAN_D, self).__init__()\n",
        "\n",
        "        self.reduced = reduced\n",
        "        self.input_dim = 3 if reduced else 4\n",
        "\n",
        "        if lstm_actv == 'tanh':\n",
        "            self.lstm_actv = nn.Tanh()\n",
        "        else:\n",
        "            self.lstm_actv = nn.Identity()\n",
        "        \n",
        "        self.lstm_1 = nn.LSTM(input_size=self.input_dim, hidden_size=500, batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=3000, hidden_size=100, batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm3 = nn.LSTM(input_size=100, hidden_size=100, batch_first=True)\n",
        "        self.bn3 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm4 = nn.LSTM(input_size=100, hidden_size=100, batch_first=True)\n",
        "        self.bn4 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm5 = nn.LSTM(input_size=100, hidden_size=1, batch_first=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Pre-trained VGG16 model\n",
        "        self.vgg = models.vgg16(weights='DEFAULT')\n",
        "        self.vgg_features = nn.Sequential(*list(self.vgg.features.children())[:])\n",
        "        for param in self.vgg_features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.conv = nn.Conv2d(512, 100, kernel_size=3, stride=1, padding=0)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.3)\n",
        "        # self.flatten = nn.Flatten()\n",
        "\n",
        "        # if weights is not None:\n",
        "        #     self.load_state_dict(torch.load(weights))\n",
        "\n",
        "\n",
        "    def forward(self, x, img_input):\n",
        "        # Scanpath input\n",
        "        print(\"in:\", x.shape)\n",
        "        x, (h1, c1) = self.lstm_1(x)\n",
        "        x = self.lstm_actv(x)\n",
        "        # print(x)\n",
        "        x = self.bn1(x)\n",
        "        print(\"bn1:\", x.shape)\n",
        "\n",
        "        # Image input\n",
        "        z = self.vgg_features(img_input)\n",
        "        print(\"z1:\", z.shape)\n",
        "        z = self.conv(z)\n",
        "        print(\"z2:\", z.shape)\n",
        "        z = self.leaky_relu(z)\n",
        "        print(\"z3:\",z.shape)\n",
        "        z = z.view(z.shape[0], -1)\n",
        "        print(\"z4 (after flatten):\",z.shape)\n",
        "        z = z.unsqueeze(1).repeat(1, 32, 1)\n",
        "        print(\"z5:\",z.shape)\n",
        "\n",
        "        # Merge\n",
        "        print(\"Before merge (x,z)\", x.shape, z.shape)\n",
        "        x = torch.cat([x, z], dim=-1)\n",
        "        print(\"cat shape:\", x.shape)\n",
        "        x, (h2, c2) = self.lstm2(x) \n",
        "        # Not passing (h1, c1) in above line to let the initial LSTM be independent from the image input\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn2(x)\n",
        "        print(x.shape)\n",
        "        x, (h3, c3) = self.lstm3(x, (h2, c2))\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn3(x)\n",
        "        print(x.shape)\n",
        "        x, (h4, c4) = self.lstm4(x, (h3, c3))\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        print(\"flow encoder out shape: \", x.shape)\n",
        "\n",
        "        # NOTE:\n",
        "        # Remove these two layers. Use 1024 x 50 features for flow encoder.\n",
        "\n",
        "        # x, _ = self.lstm5(x)\n",
        "        # x = self.lstm_actv(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.sigmoid(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        return x, (h4, c4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxZu398stHIi",
        "outputId": "05a05633-f96f-48a6-bedf-b1b4a2e9aebf"
      },
      "outputs": [],
      "source": [
        "model = PathGAN_D(reduced=True, in_vecs=32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhtF1WWtI4S",
        "outputId": "c819041c-560f-46af-a306-fad10af82543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in: torch.Size([16, 32, 3])\n",
            "bn1: torch.Size([16, 32, 500])\n",
            "z1: torch.Size([16, 512, 7, 7])\n",
            "z2: torch.Size([16, 100, 5, 5])\n",
            "z3: torch.Size([16, 100, 5, 5])\n",
            "z4 (after flatten): torch.Size([16, 2500])\n",
            "z5: torch.Size([16, 32, 2500])\n",
            "Before merge (x,z) torch.Size([16, 32, 500]) torch.Size([16, 32, 2500])\n",
            "cat shape: torch.Size([16, 32, 3000])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "flow encoder out shape:  torch.Size([16, 32, 100])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(16, 32, 3).to(device) # batch_size, seq_len, feature_dim. NOTE: Seq_len can be variable.\n",
        "img_input = torch.randn(16, 3, 224, 224).to(device)\n",
        "\n",
        "out, hidden_embeddings = model(x, img_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 100])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape Reducer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOTE: Only this gets trained in the flow-prompt-Adapter (FP-Adapter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class shapeReducerMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(shapeReducerMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        self.linear1 = nn.Linear(3200, 1600)\n",
        "        self.linear2 = nn.Linear(1600, 1024)\n",
        "        self.model = nn.Sequential(\n",
        "            self.linear1,\n",
        "            nn.LeakyReLU(0.3),\n",
        "            self.linear2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x) # BS x 3200\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "shape_reducer = shapeReducerMLP().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 1024])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans = shape_reducer(out)\n",
        "ans.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shallow Decoder for flowEncoder (PathGAN-D+ShapeReducerMLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class pathGAN_D_Decoder(nn.Module):\n",
        "    def __init__(self, out_dim=3,  in_vecs = 32, latent_dim = 1024): # latent dim always fixed at 1024\n",
        "        super(pathGAN_D_Decoder, self).__init__()\n",
        "        \n",
        "        assert latent_dim % in_vecs == 0, \"Latent dim should be divisible by in_vecs\"\n",
        "        # TODO: \n",
        "        # step 0: Get the input/output shapes right\n",
        "        # step 1: Reshape BS x 1024 to BS x in_vecs x -1\n",
        "        # step 2: LSTM decoding --> predict sequence of in_vecs x 3 (scanpaths) for entire BS\n",
        "\n",
        "        self.out_dim = out_dim # Should be same as input_dim of PathGAN_D\n",
        "        self.latent_dim = latent_dim // in_vecs\n",
        "        print(\"Latent dim:\", self.latent_dim)\n",
        "        self.in_vecs = in_vecs\n",
        "        \n",
        "        self.linear_init = nn.Linear(1, in_vecs)\n",
        "        self.linear_init_actv = nn.LeakyReLU(0.3)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(input_size=self.latent_dim, \n",
        "                             hidden_size=self.latent_dim // 4, \n",
        "                             batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=self.latent_dim // 4, \n",
        "                             hidden_size=self.latent_dim // 4, \n",
        "                             batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.linear = nn.Linear(self.latent_dim // 4, out_dim)\n",
        "        \n",
        "        self.lstm_actv = nn.Tanh()\n",
        "\n",
        "        # Final shape has to be [BS x in_vecs x 3]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, None, :] # BS x 1 x 1024\n",
        "\n",
        "        # NOTE: Is this the right way to get the output prediction?\n",
        "        x = x.view(x.shape[0], self.in_vecs, -1) # BS x in_vecs x (1024/in_vecs)\n",
        "\n",
        "        print(\"Before all layers\", x.shape)\n",
        "        # x = self.linear_init(x) # \n",
        "        # print(\"Init. linear\", x.shape)\n",
        "        # x = self.linear_init_actv(x)\n",
        "\n",
        "        print(\"Starting the Sequence decoder ...\")\n",
        "        x, (h0, c0) = self.lstm1(x)\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        # print(x.shape)\n",
        "        x = self.bn1(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x, (h1, c1) = self.lstm2(x, (h0, c0))\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn2(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        # x, (h2, c2) = self.lstm3(x, (h1, c1))\n",
        "        # print(x.shape)\n",
        "        # x = self.lstm_actv(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.bn3(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # x, (h3, c3) = self.lstm4(x, (h2, c2))\n",
        "        # print(x.shape)\n",
        "        # x = self.lstm_actv(x)\n",
        "        # print(x.shape)\n",
        "        x = self.linear(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latent dim: 32\n"
          ]
        }
      ],
      "source": [
        "disc_decoder = pathGAN_D_Decoder(out_dim=3, in_vecs=32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before all layers torch.Size([16, 32, 32])\n",
            "Starting the Sequence decoder ...\n",
            "torch.Size([16, 32, 8])\n",
            "torch.Size([16, 32, 8])\n",
            "torch.Size([16, 32, 8])\n",
            "torch.Size([16, 32, 8])\n",
            "torch.Size([16, 32, 3])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 3])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reconstructed = disc_decoder(ans)\n",
        "reconstructed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS_JMMzbZubF"
      },
      "source": [
        "## CLIPVisionWithProjection - Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE81Vpvqat5_",
        "outputId": "c69b22f0-993a-49a8-8375-bb0924220e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install -Uqq transformers accelerate diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "owakWwVLaslv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/researcher/anaconda3/envs/control/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-07-24 17:01:31.295692: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-24 17:01:31.316828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-24 17:01:31.651924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sRdCxpJMZ3V_"
      },
      "outputs": [],
      "source": [
        "class ImageProjModel(torch.nn.Module):\n",
        "    \"\"\"Project for Cross-Attenuation with Text Embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generator = None\n",
        "        self.cross_attention_dim = cross_attention_dim\n",
        "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
        "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
        "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
        "\n",
        "    def forward(self, image_embeds):\n",
        "        embeds = image_embeds\n",
        "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
        "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
        "        )\n",
        "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
        "        return clip_extra_context_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fz2_p0p7sJSe"
      },
      "outputs": [],
      "source": [
        "# !mkdir models/image_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r2QVLQ1vh2Qk"
      },
      "outputs": [],
      "source": [
        "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "image_encoder_path = \"./models/image_encoder\"\n",
        "# ip_ckpt = \"models/ip-adapter_sd15.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "KfEWOGORHyFp",
        "outputId": "82231fc1-3c2b-4b9b-d755-d2fcff1ee6f8"
      },
      "outputs": [],
      "source": [
        "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
        "clip_image_processor = CLIPImageProcessor()\n",
        "image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmqZHlWjZ3yd"
      },
      "outputs": [],
      "source": [
        "image_proj_model = ImageProjModel(\n",
        "        cross_attention_dim=unet.config.cross_attention_dim,\n",
        "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
        "        clip_extra_context_tokens=4,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_encoder.config.projection_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MTAgQEOjaH9_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([1, 1024])\n",
            "2 torch.Size([1, 1024])\n"
          ]
        }
      ],
      "source": [
        "image_path = \"test_img.png\"\n",
        "raw_image = Image.open(image_path)\n",
        "clip_image = clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
        "image_embeds = image_encoder(clip_image).image_embeds\n",
        "print(\"1\", image_embeds.shape)\n",
        "image_embeds_ = []\n",
        "for image_embed, drop_image_embed in zip(image_embeds, [0]*len(image_embeds)):\n",
        "  if drop_image_embed == 1:\n",
        "    image_embeds_.append(torch.zeros_like(image_embed))\n",
        "  else:\n",
        "    image_embeds_.append(image_embed)\n",
        "\n",
        "image_embeds = torch.stack(image_embeds_)\n",
        "print(\"2\", image_embeds.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
