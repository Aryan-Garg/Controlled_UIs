{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8DkUEIRdAh8"
      },
      "source": [
        "## Path GAN Discriminator Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xsxwyx7_sqYo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "RXjk82ZWso7E"
      },
      "outputs": [],
      "source": [
        "class PathGAN_D(nn.Module):\n",
        "    def __init__(self,\n",
        "                 reduced, in_vecs = 32, lstm_actv='tanh'):\n",
        "        super(PathGAN_D, self).__init__()\n",
        "\n",
        "        self.reduced = reduced\n",
        "        self.input_dim = 3 if reduced else 4\n",
        "\n",
        "        if lstm_actv == 'tanh':\n",
        "            self.lstm_actv = nn.Tanh()\n",
        "        else:\n",
        "            self.lstm_actv = nn.Identity()\n",
        "        \n",
        "        self.lstm_1 = nn.LSTM(input_size=self.input_dim, hidden_size=500, batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=3000, hidden_size=100, batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm3 = nn.LSTM(input_size=100, hidden_size=100, batch_first=True)\n",
        "        self.bn3 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm4 = nn.LSTM(input_size=100, hidden_size=100, batch_first=True)\n",
        "        self.bn4 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm5 = nn.LSTM(input_size=100, hidden_size=1, batch_first=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Pre-trained VGG16 model\n",
        "        self.vgg = models.vgg16(weights='DEFAULT')\n",
        "        self.vgg_features = nn.Sequential(*list(self.vgg.features.children())[:])\n",
        "        for param in self.vgg_features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.conv = nn.Conv2d(512, 100, kernel_size=3, stride=1, padding=0)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.3)\n",
        "        # self.flatten = nn.Flatten()\n",
        "\n",
        "        # if weights is not None:\n",
        "        #     self.load_state_dict(torch.load(weights))\n",
        "\n",
        "\n",
        "    def forward(self, x, img_input):\n",
        "        # Scanpath input\n",
        "        print(\"in:\", x.shape)\n",
        "        x, (h1, c1) = self.lstm_1(x)\n",
        "        x = self.lstm_actv(x)\n",
        "        # print(x)\n",
        "        x = self.bn1(x)\n",
        "        print(\"bn1:\", x.shape)\n",
        "\n",
        "        # Image input\n",
        "        z = self.vgg_features(img_input)\n",
        "        print(\"z1:\", z.shape)\n",
        "        z = self.conv(z)\n",
        "        print(\"z2:\", z.shape)\n",
        "        z = self.leaky_relu(z)\n",
        "        print(\"z3:\",z.shape)\n",
        "        z = z.view(z.shape[0], -1)\n",
        "        print(\"z4 (after flatten):\",z.shape)\n",
        "        z = z.unsqueeze(1).repeat(1, 32, 1)\n",
        "        print(\"z5:\",z.shape)\n",
        "\n",
        "        # Merge\n",
        "        print(\"Before merge (x,z)\", x.shape, z.shape)\n",
        "        x = torch.cat([x, z], dim=-1)\n",
        "        print(\"cat shape:\", x.shape)\n",
        "        x, (h2, c2) = self.lstm2(x) \n",
        "        # Not passing (h1, c1) in above line to let the initial LSTM be independent from the image input\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn2(x)\n",
        "        print(x.shape)\n",
        "        x, (h3, c3) = self.lstm3(x, (h2, c2))\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn3(x)\n",
        "        print(x.shape)\n",
        "        x, (h4, c4) = self.lstm4(x, (h3, c3))\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn4(x)\n",
        "\n",
        "        print(\"flow encoder out shape: \", x.shape)\n",
        "\n",
        "        # NOTE:\n",
        "        # Remove these two layers. Use 1024 x 50 features for flow encoder.\n",
        "\n",
        "        # x, _ = self.lstm5(x)\n",
        "        # x = self.lstm_actv(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.sigmoid(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        return x, (h4, c4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxZu398stHIi",
        "outputId": "05a05633-f96f-48a6-bedf-b1b4a2e9aebf"
      },
      "outputs": [],
      "source": [
        "model = PathGAN_D(reduced=True, in_vecs=32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhtF1WWtI4S",
        "outputId": "c819041c-560f-46af-a306-fad10af82543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in: torch.Size([16, 32, 3])\n",
            "bn1: torch.Size([16, 32, 500])\n",
            "z1: torch.Size([16, 512, 7, 7])\n",
            "z2: torch.Size([16, 100, 5, 5])\n",
            "z3: torch.Size([16, 100, 5, 5])\n",
            "z4 (after flatten): torch.Size([16, 2500])\n",
            "z5: torch.Size([16, 32, 2500])\n",
            "Before merge (x,z) torch.Size([16, 32, 500]) torch.Size([16, 32, 2500])\n",
            "cat shape: torch.Size([16, 32, 3000])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "torch.Size([16, 32, 100])\n",
            "flow encoder out shape:  torch.Size([16, 32, 100])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(16, 32, 3).to(device) # batch_size, seq_len, feature_dim. NOTE: Seq_len can be variable.\n",
        "img_input = torch.randn(16, 3, 224, 224).to(device)\n",
        "\n",
        "out, hidden_embeddings = model(x, img_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 32, 100])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape Reducer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NOTE: Only this gets trained in the flow-prompt-Adapter (FP-Adapter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "class shapeReducerMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(shapeReducerMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "        self.linear1 = nn.Linear(3200, 1600)\n",
        "        self.linear2 = nn.Linear(1600, 1024)\n",
        "        self.model = nn.Sequential(\n",
        "            self.linear1,\n",
        "            nn.LeakyReLU(0.3),\n",
        "            self.linear2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x) # BS x 3200\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "shape_reducer = shapeReducerMLP().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 1024])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans = shape_reducer(out)\n",
        "ans.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shallow Decoder for flowEncoder (PathGAN-D+ShapeReducerMLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class pathGAN_D_Decoder(nn.Module):\n",
        "    def __init__(self, out_dim,  in_vecs = 32, latent_dim = 1024): # latent dim always fixed at 1024\n",
        "        super(pathGAN_D_Decoder, self).__init__()\n",
        "\n",
        "        # TODO: \n",
        "        # step 0: Get the input/output shapes right\n",
        "        # step 1: Reshape BS x 1024 to BS x in_vecs x -1\n",
        "        # step 2: LSTM decoding --> predict sequence of in_vecs x 3 (scanpaths) for entire BS\n",
        "\n",
        "        self.out_dim = out_dim # Should be same as input_dim of PathGAN_D\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # self.linear_init = nn.Linear(1, in_vecs)\n",
        "        # self.linear_init_actv = nn.LeakyReLU(0.3)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size=latent_dim, hidden_size=latent_dim // 4, batch_first=True)\n",
        "        self.bn1 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm2 = nn.LSTM(input_size=latent_dim // 4, hidden_size=latent_dim // 16, batch_first=True)\n",
        "        self.bn2 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm3 = nn.LSTM(input_size=latent_dim // 16, hidden_size=latent_dim // 64, batch_first=True)\n",
        "        self.bn3 = nn.BatchNorm1d(in_vecs)\n",
        "\n",
        "        self.lstm4 = nn.LSTM(input_size=latent_dim // 64, hidden_size=out_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(out_dim, out_dim)\n",
        "\n",
        "        self.lstm_actv = nn.Tanh()\n",
        "\n",
        "        # self.model = nn.Sequential(\n",
        "        #     self.lstm1,\n",
        "        #     self.lstm_actv,\n",
        "        #     self.bn1,\n",
        "        #     self.lstm2,\n",
        "        #     self.lstm_actv,\n",
        "        #     self.bn2,\n",
        "        #     self.lstm3,\n",
        "        #     self.lstm_actv,\n",
        "        #     self.bn3,\n",
        "        #     self.lstm4,\n",
        "        #     self.lstm_actv,\n",
        "        #     self.linear,\n",
        "        # )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        print(\"Before all layers\", x.shape)\n",
        "        x = self.linear_init(x)\n",
        "        print(\"Init. linear\", x.shape)\n",
        "        x = self.linear_init_actv(x)\n",
        "        x = x.transpose(1, 0)\n",
        "        print(x.shape)\n",
        "\n",
        "        print(\"Starting the Sequence decoder ...\")\n",
        "        x, (h0, c0) = self.lstm1(x)\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn1(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x, (h1, c1) = self.lstm2(x, (h0, c0))\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn2(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x, (h2, c2) = self.lstm3(x, (h1, c1))\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.bn3(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        x, (h3, c3) = self.lstm4(x, (h2, c2))\n",
        "        print(x.shape)\n",
        "        x = self.lstm_actv(x)\n",
        "        print(x.shape)\n",
        "        x = self.linear(x)\n",
        "        print(x.shape)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "disc_decoder = pathGAN_D_Decoder(out_dim=3, in_vecs=32).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before all layers torch.Size([1024, 1])\n",
            "Init. linear torch.Size([1024, 32])\n",
            "torch.Size([32, 1024])\n",
            "Start the Sequence decoder\n",
            "torch.Size([32, 256])\n",
            "torch.Size([32, 256])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 256 elements not 32",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mdisc_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m reconstructed\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[59], line 53\u001b[0m, in \u001b[0;36mpathGAN_D_Decoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_actv(x)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     56\u001b[0m x, (h1, c1) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(x, (h0, c0))\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/control/lib/python3.8/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 256 elements not 32"
          ]
        }
      ],
      "source": [
        "reconstructed = disc_decoder(ans.transpose(1,0)).transpose(1, 0)\n",
        "reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS_JMMzbZubF"
      },
      "source": [
        "## CLIPVisionWithProjection - Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE81Vpvqat5_",
        "outputId": "c69b22f0-993a-49a8-8375-bb0924220e80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install -Uqq transformers accelerate diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "owakWwVLaslv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/researcher/anaconda3/envs/control/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-07-24 17:01:31.295692: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-24 17:01:31.316828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-24 17:01:31.651924: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPImageProcessor\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sRdCxpJMZ3V_"
      },
      "outputs": [],
      "source": [
        "class ImageProjModel(torch.nn.Module):\n",
        "    \"\"\"Project for Cross-Attenuation with Text Embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.generator = None\n",
        "        self.cross_attention_dim = cross_attention_dim\n",
        "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
        "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
        "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
        "\n",
        "    def forward(self, image_embeds):\n",
        "        embeds = image_embeds\n",
        "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
        "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
        "        )\n",
        "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
        "        return clip_extra_context_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fz2_p0p7sJSe"
      },
      "outputs": [],
      "source": [
        "# !mkdir models/image_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "r2QVLQ1vh2Qk"
      },
      "outputs": [],
      "source": [
        "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "image_encoder_path = \"./models/image_encoder\"\n",
        "# ip_ckpt = \"models/ip-adapter_sd15.bin\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "KfEWOGORHyFp",
        "outputId": "82231fc1-3c2b-4b9b-d755-d2fcff1ee6f8"
      },
      "outputs": [],
      "source": [
        "unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
        "clip_image_processor = CLIPImageProcessor()\n",
        "image_encoder = CLIPVisionModelWithProjection.from_pretrained(image_encoder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmqZHlWjZ3yd"
      },
      "outputs": [],
      "source": [
        "image_proj_model = ImageProjModel(\n",
        "        cross_attention_dim=unet.config.cross_attention_dim,\n",
        "        clip_embeddings_dim=image_encoder.config.projection_dim,\n",
        "        clip_extra_context_tokens=4,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_encoder.config.projection_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MTAgQEOjaH9_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 torch.Size([1, 1024])\n",
            "2 torch.Size([1, 1024])\n"
          ]
        }
      ],
      "source": [
        "image_path = \"test_img.png\"\n",
        "raw_image = Image.open(image_path)\n",
        "clip_image = clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
        "image_embeds = image_encoder(clip_image).image_embeds\n",
        "print(\"1\", image_embeds.shape)\n",
        "image_embeds_ = []\n",
        "for image_embed, drop_image_embed in zip(image_embeds, [0]*len(image_embeds)):\n",
        "  if drop_image_embed == 1:\n",
        "    image_embeds_.append(torch.zeros_like(image_embed))\n",
        "  else:\n",
        "    image_embeds_.append(image_embed)\n",
        "\n",
        "image_embeds = torch.stack(image_embeds_)\n",
        "print(\"2\", image_embeds.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
