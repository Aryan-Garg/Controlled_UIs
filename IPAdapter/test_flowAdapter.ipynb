{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowAdapter M2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import ruamel.yaml as yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "\n",
    "from ip_adapter.ip_adapter import ImageProjModel\n",
    "from ip_adapter.utils import is_torch2_available\n",
    "if is_torch2_available():\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\n",
    "\n",
    "from models_eyeformer.model_tracking import TrackingTransformer\n",
    "from pytorchSoftdtwCuda.soft_dtw_cuda import SoftDTW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Encoders & Projectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlowEncoder_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_module('encoder', nn.Sequential(\n",
    "            nn.Linear(45, 128),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(512, 1024)))\n",
    "\n",
    "        self.add_module('decoder', nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(128, 45)))\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.encoder(x)\n",
    "      x = self.decoder(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowEncoder(nn.Module):\n",
    "    def __init__(self, cross_attention_dim, clip_embeddings_dim, clip_extra_context_tokens, \n",
    "                 eyeFormer, flow_latenizer):\n",
    "        super().__init__()\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_embeddings_dim = clip_embeddings_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.eyeFormer = eyeFormer\n",
    "        self.flow_latenizer = flow_latenizer\n",
    "    \n",
    "    def forward(self, images):\n",
    "        if self.eyeFormer is not None:\n",
    "            flow_embeds = self.eyeFormer(images)\n",
    "        else:\n",
    "            flow_embeds = images\n",
    "        \n",
    "        flow_embeds = flow_embeds.view(flow_embeds.size(0), -1)\n",
    "        flow_embeds = self.flow_latenizer(flow_embeds)\n",
    "        # print(\"Final flow_embeds Shape: \", flow_embeds.shape)\n",
    "        return flow_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectProjModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Correct the final flow embedding wiht a linear norm and final projection layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
    "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
    "        )\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, json_file, tokenizer, size=256, \n",
    "                 t_drop_rate=0.05, i_drop_rate=0.05, \n",
    "                 ti_drop_rate=0.05, dataset_name=\"ueyes\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.i_drop_rate = i_drop_rate\n",
    "        self.t_drop_rate = t_drop_rate\n",
    "        self.ti_drop_rate = ti_drop_rate\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        self.data = json.load(open(json_file)) # list of dict: [{\"image_file\": \"1.png\", \"text\": \"A dog\"}]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        # self.clip_image_processor = CLIPImageProcessor()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx] \n",
    "        text = item[\"prompt\"]\n",
    "        image_file = item[\"target\"]\n",
    "        \n",
    "        # read image and flow vector\n",
    "        raw_image = Image.open(image_file)\n",
    "        image = self.transform(raw_image.convert(\"RGB\"))\n",
    "\n",
    "        # clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # drop\n",
    "        drop_flow_embed = 0\n",
    "        rand_num = random.random()\n",
    "        if rand_num < self.i_drop_rate:\n",
    "            drop_flow_embed = 1\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate):\n",
    "            text = \"\"\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate + self.ti_drop_rate):\n",
    "            text = \"\"\n",
    "            drop_flow_embed = 1\n",
    "        # get text and tokenize\n",
    "        text_input_ids = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        # Added for flow-Adapter:\n",
    "        if self.dataset_name == \"ueyes\": # NOT Implemented yet\n",
    "            flow_input = item[\"flow_input\"]\n",
    "        else:\n",
    "            flow_input = None\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            # \"clip_image\": clip_image,\n",
    "            \"drop_flow_embed\": drop_flow_embed,\n",
    "            \"flow_input\": flow_input\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn(data):\n",
    "    images = torch.stack([example[\"image\"] for example in data])\n",
    "    text_input_ids = torch.cat([example[\"text_input_ids\"] for example in data], dim=0)\n",
    "    # clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n",
    "    drop_flow_embeds = [example[\"drop_flow_embed\"] for example in data]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        # \"clip_images\": clip_images,\n",
    "        \"drop_flow_embeds\": drop_flow_embeds\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPAdapter (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(seed, device):\n",
    "\n",
    "    if seed is not None:\n",
    "        if isinstance(seed, list):\n",
    "            generator = [torch.Generator(device).manual_seed(seed_item) for seed_item in seed]\n",
    "        else:\n",
    "            generator = torch.Generator(device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "\n",
    "    return generator\n",
    "\n",
    "from typing import List\n",
    "from diffusers.pipelines.controlnet import MultiControlNetModel\n",
    "from ip_adapter.attention_processor import AttnProcessor, CNAttnProcessor, IPAttnProcessor\n",
    "class IPAdapter(torch.nn.Module):\n",
    "    \"\"\"Custom variation\"\"\"\n",
    "    def __init__(self, pipe, correct_proj_model, ckpt_path=None, num_tokens=4):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pipe = pipe.to(self.device)\n",
    "        self.correct_proj_model = correct_proj_model.to(self.device)\n",
    "        self.adapter_modules = self.set_ip_adapter()\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_ip_adapter(ckpt_path)\n",
    "\n",
    "        \n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                attn_procs[name] = AttnProcessor()\n",
    "            else:\n",
    "                attn_procs[name] = IPAttnProcessor(\n",
    "                    hidden_size=hidden_size,\n",
    "                    cross_attention_dim=cross_attention_dim,\n",
    "                    scale=1.0,\n",
    "                    num_tokens=self.num_tokens,\n",
    "                ).to(self.device, dtype=torch.float16)\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "        if hasattr(self.pipe, \"controlnet\"):\n",
    "            if isinstance(self.pipe.controlnet, MultiControlNetModel):\n",
    "                for controlnet in self.pipe.controlnet.nets:\n",
    "                    controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "            else:\n",
    "                self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, flow_embeds):\n",
    "        # print(\"flow_embeds shape: \", flow_embeds.shape)\n",
    "        # print(\"encoder_hidden_states shape: \", encoder_hidden_states.shape)\n",
    "        ip_tokens = self.correct_proj_model(flow_embeds)\n",
    "        # print(\"ip_tokens shape: \", ip_tokens.shape)\n",
    "        \n",
    "        encoder_hidden_states = torch.cat([encoder_hidden_states, ip_tokens], dim=1)\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "\n",
    "    def load_ip_adapter(self, ckpt_path):\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        self.correct_proj_model.load_state_dict(state_dict[\"flow_proj\"])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict[\"ip_adapter\"])\n",
    "        print(f\"Successfully loaded IP Adapter from checkpoint {ckpt_path}\")\n",
    "    \n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.correct_proj_model.parameters()]))\n",
    "        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for correct_proj_model and adapter_modules\n",
    "        self.correct_proj_model.load_state_dict(state_dict[\"flow_proj\"], strict=True)\n",
    "        self.adapter_modules.load_state_dict(state_dict[\"ip_adapter\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.correct_proj_model.parameters()]))\n",
    "        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_ip_proj_sum != new_ip_proj_sum, \"Weights of correct_proj_model did not change!\"\n",
    "        assert orig_adapter_sum != new_adapter_sum, \"Weights of adapter_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")\n",
    "\n",
    "\n",
    "    # TODO: Fix this with EyeFormer and the flow_latenizer stuff instead of CLIP\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image=None, clip_image_embeds=None):\n",
    "        if pil_image is not None:\n",
    "            if isinstance(pil_image, Image.Image):\n",
    "                pil_image = [pil_image]\n",
    "            clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "            clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtype=torch.float16)).image_embeds\n",
    "        else:\n",
    "            clip_image_embeds = clip_image_embeds.to(self.device, dtype=torch.float16)\n",
    "        image_prompt_embeds = self.correct_proj_model(clip_image_embeds)\n",
    "        uncond_image_prompt_embeds = self.correct_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "\n",
    "    # TODO: Fix this with EyeFormer and the flow_latenizer stuff instead of CLIP\n",
    "    def generate(\n",
    "        self,\n",
    "        pil_image=None,\n",
    "        clip_image_embeds=None,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        num_samples=4,\n",
    "        seed=None,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=30,\n",
    "        **kwargs):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        if pil_image is not None:\n",
    "            num_prompts = 1 if isinstance(pil_image, Image.Image) else len(pil_image)\n",
    "        else:\n",
    "            num_prompts = clip_image_embeds.size(0)\n",
    "\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality, user interfaces\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(\n",
    "            pil_image=pil_image, clip_image_embeds=clip_image_embeds\n",
    "        )\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds_, negative_prompt_embeds_ = self.pipe.encode_prompt(\n",
    "                prompt,\n",
    "                device=self.device,\n",
    "                num_images_per_prompt=num_samples,\n",
    "                do_classifier_free_guidance=True,\n",
    "                negative_prompt=negative_prompt,\n",
    "            )\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = get_generator(seed, self.device)\n",
    "\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args & Other Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pretrained_ip_adapter = \"./sd-flow_adapter/custom_model_checkpoint-112000.pt\"\n",
    "    pretrained_flow_latenizer = \"./sd-flow_adapter/checkpoint-112000/pytorch_model_1.bin\"\n",
    "    data_json_file = \"/home/researcher/Documents/dataset/original_datasets/webui_prompts.json\"\n",
    "    resolution = 256\n",
    "    dataloader_num_workers = 8\n",
    "    dataset_name = \"everything_else\"\n",
    "    output_dir = \"./test_flowAdapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will generate 16 points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=45, out_features=128, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.3)\n",
       "  (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.3)\n",
       "  (4): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.3)\n",
       "  (6): Linear(in_features=512, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "# attn_procs = {}\n",
    "# unet_sd = unet.state_dict()\n",
    "# for name in unet.attn_processors.keys():\n",
    "#     cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "#     if name.startswith(\"mid_block\"):\n",
    "#         hidden_size = unet.config.block_out_channels[-1]\n",
    "#     elif name.startswith(\"up_blocks\"):\n",
    "#         block_id = int(name[len(\"up_blocks.\")])\n",
    "#         hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "#     elif name.startswith(\"down_blocks\"):\n",
    "#         block_id = int(name[len(\"down_blocks.\")])\n",
    "#         hidden_size = unet.config.block_out_channels[block_id]\n",
    "#     if cross_attention_dim is None:\n",
    "#         attn_procs[name] = AttnProcessor()\n",
    "#     else:\n",
    "#         layer_name = name.split(\".processor\")[0]\n",
    "#         weights = {\n",
    "#             \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "#             \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "#         }\n",
    "#         attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, \n",
    "#                                            cross_attention_dim=cross_attention_dim)\n",
    "#         attn_procs[name].load_state_dict(weights)\n",
    "# unet.set_attn_processor(attn_procs)\n",
    "# adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "\n",
    "flowAE = FlowEncoder_MLP()\n",
    "flow_latenizer = flowAE.encoder\n",
    "\n",
    "if args.dataset_name == \"ueyes\":\n",
    "    flowAE.load_state_dict(torch.load(\"/home/researcher/flowAE.pth\")) # TODO: Put the right path here\n",
    "    eyeFormer = None\n",
    "else: \n",
    "    config = yaml.load(open(\"./configs/Tracking.yaml\", 'r'), Loader=yaml.Loader)\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))\n",
    "    eyeFormer = TrackingTransformer(config = config, init_deit=False)\n",
    "    checkpointEF = torch.load(\"/home/researcher/Documents/aryan/asciProject/flowEncoder/weights/checkpoint_19.pth\",\n",
    "                            map_location='cpu')\n",
    "    state_dict = checkpointEF['model']\n",
    "    eyeFormer.load_state_dict(state_dict)\n",
    "    eyeFormer.requires_grad_(False)\n",
    "    \n",
    "flow_latenizer.requires_grad_(False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['flow_latenizer.0.weight', 'flow_latenizer.0.bias', 'flow_latenizer.2.weight', 'flow_latenizer.2.bias', 'flow_latenizer.4.weight', 'flow_latenizer.4.bias', 'flow_latenizer.6.weight', 'flow_latenizer.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "f =  torch.load(args.pretrained_flow_latenizer)\n",
    "print(f.keys()) \n",
    "fix_flow_latenizer = {}\n",
    "for key in f.keys():\n",
    "    fix_flow_latenizer[key.replace(\"flow_latenizer.\", \"\")] = f[key]\n",
    "flow_latenizer.load_state_dict(fix_flow_latenizer)\n",
    "\n",
    "flow_projection_model = CorrectProjModel(\n",
    "            cross_attention_dim=unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=1024,\n",
    "            clip_extra_context_tokens=4)\n",
    "# model_ckpt = torch.load(\"./sd-flow_adapter/custom_model_checkpoint-112000.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  4.18it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    args.pretrained_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded IP Adapter from checkpoint ./sd-flow_adapter/custom_model_checkpoint-112000.pt\n"
     ]
    }
   ],
   "source": [
    "ip_adapter = IPAdapter(pipe, \n",
    "                       flow_projection_model,  \n",
    "                       args.pretrained_ip_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Input a flow prompt or a \n",
    "# UI image whose flow you want to use to generate new designs for\n",
    "\n",
    "# TODO - A: Handcraft a flow vector\n",
    "flow_vector = torch.randn(1, 45).to(torch.float16)\n",
    "\n",
    "# TODO - B: Use a UI image to extract flow vector\n",
    "image = Image.open(\"assets/images/woman.png\")\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image variations\n",
    "images = ip_adapter.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42)\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
