{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowAdapter M2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import ruamel.yaml as yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPImageProcessor\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "\n",
    "from ip_adapter.ip_adapter import ImageProjModel\n",
    "from ip_adapter.utils import is_torch2_available\n",
    "if is_torch2_available():\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor\n",
    "else:\n",
    "    from ip_adapter.attention_processor import IPAttnProcessor, AttnProcessor\n",
    "\n",
    "from models_eyeformer.model_tracking import TrackingTransformer\n",
    "from pytorchSoftdtwCuda.soft_dtw_cuda import SoftDTW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Encoders & Projectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlowEncoder_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_module('encoder', nn.Sequential(\n",
    "            nn.Linear(45, 128),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(512, 1024)))\n",
    "\n",
    "        self.add_module('decoder', nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Linear(128, 45)))\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = x.view(x.size(0), -1)\n",
    "      x = self.encoder(x)\n",
    "      x = self.decoder(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowEncoder(nn.Module):\n",
    "    def __init__(self, cross_attention_dim, clip_embeddings_dim, clip_extra_context_tokens, \n",
    "                 eyeFormer, flow_latenizer):\n",
    "        super().__init__()\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_embeddings_dim = clip_embeddings_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.eyeFormer = eyeFormer\n",
    "        self.flow_latenizer = flow_latenizer\n",
    "    \n",
    "    def forward(self, images):\n",
    "        if self.eyeFormer is not None:\n",
    "            flow_embeds = self.eyeFormer(images)\n",
    "        else:\n",
    "            flow_embeds = images\n",
    "        \n",
    "        flow_embeds = flow_embeds.view(flow_embeds.size(0), -1)\n",
    "        flow_embeds = self.flow_latenizer(flow_embeds)\n",
    "        # print(\"Final flow_embeds Shape: \", flow_embeds.shape)\n",
    "        return flow_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectProjModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Correct the final flow embedding wiht a linear norm and final projection layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = None\n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(\n",
    "            -1, self.clip_extra_context_tokens, self.cross_attention_dim\n",
    "        )\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, json_file, tokenizer, size=256, \n",
    "                 t_drop_rate=0.05, i_drop_rate=0.05, \n",
    "                 ti_drop_rate=0.05, dataset_name=\"ueyes\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.i_drop_rate = i_drop_rate\n",
    "        self.t_drop_rate = t_drop_rate\n",
    "        self.ti_drop_rate = ti_drop_rate\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        self.data = json.load(open(json_file)) # list of dict: [{\"image_file\": \"1.png\", \"text\": \"A dog\"}]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(self.size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "        # self.clip_image_processor = CLIPImageProcessor()\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx] \n",
    "        text = item[\"prompt\"]\n",
    "        image_file = item[\"target\"]\n",
    "        \n",
    "        # read image and flow vector\n",
    "        raw_image = Image.open(image_file)\n",
    "        image = self.transform(raw_image.convert(\"RGB\"))\n",
    "\n",
    "        # clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # drop\n",
    "        drop_flow_embed = 0\n",
    "        rand_num = random.random()\n",
    "        if rand_num < self.i_drop_rate:\n",
    "            drop_flow_embed = 1\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate):\n",
    "            text = \"\"\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate + self.ti_drop_rate):\n",
    "            text = \"\"\n",
    "            drop_flow_embed = 1\n",
    "        # get text and tokenize\n",
    "        text_input_ids = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        \n",
    "        # Added for flow-Adapter:\n",
    "        if self.dataset_name == \"ueyes\": # NOT Implemented yet\n",
    "            flow_input = item[\"flow_input\"]\n",
    "        else:\n",
    "            flow_input = None\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            # \"clip_image\": clip_image,\n",
    "            \"drop_flow_embed\": drop_flow_embed,\n",
    "            \"flow_input\": flow_input\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "def collate_fn(data):\n",
    "    images = torch.stack([example[\"image\"] for example in data])\n",
    "    text_input_ids = torch.cat([example[\"text_input_ids\"] for example in data], dim=0)\n",
    "    # clip_images = torch.cat([example[\"clip_image\"] for example in data], dim=0)\n",
    "    drop_flow_embeds = [example[\"drop_flow_embed\"] for example in data]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        # \"clip_images\": clip_images,\n",
    "        \"drop_flow_embeds\": drop_flow_embeds\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPAdapter (Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAdapter(torch.nn.Module):\n",
    "    \"\"\"Custom variation\"\"\"\n",
    "    def __init__(self, unet, correct_proj_model, adapter_modules, ckpt_path=None):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.correct_proj_model = correct_proj_model\n",
    "        self.adapter_modules = adapter_modules\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, flow_embeds):\n",
    "        # print(\"flow_embeds shape: \", flow_embeds.shape)\n",
    "        # print(\"encoder_hidden_states shape: \", encoder_hidden_states.shape)\n",
    "        ip_tokens = self.correct_proj_model(flow_embeds)\n",
    "        # print(\"ip_tokens shape: \", ip_tokens.shape)\n",
    "        \n",
    "        encoder_hidden_states = torch.cat([encoder_hidden_states, ip_tokens], dim=1)\n",
    "        # Predict the noise residual\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n",
    "\n",
    "    def load_from_checkpoint(self, ckpt_path: str):\n",
    "        # Calculate original checksums\n",
    "        orig_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.correct_proj_model.parameters()]))\n",
    "        orig_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "        # Load state dict for correct_proj_model and adapter_modules\n",
    "        self.correct_proj_model.load_state_dict(state_dict[\"image_proj\"], strict=True)\n",
    "        self.adapter_modules.load_state_dict(state_dict[\"ip_adapter\"], strict=True)\n",
    "\n",
    "        # Calculate new checksums\n",
    "        new_ip_proj_sum = torch.sum(torch.stack([torch.sum(p) for p in self.correct_proj_model.parameters()]))\n",
    "        new_adapter_sum = torch.sum(torch.stack([torch.sum(p) for p in self.adapter_modules.parameters()]))\n",
    "\n",
    "        # Verify if the weights have changed\n",
    "        assert orig_ip_proj_sum != new_ip_proj_sum, \"Weights of correct_proj_model did not change!\"\n",
    "        assert orig_adapter_sum != new_adapter_sum, \"Weights of adapter_modules did not change!\"\n",
    "\n",
    "        print(f\"Successfully loaded weights from checkpoint {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args & Other Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS:\n",
    "    pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pretrained_ip_adapter_path = \"./sd-flow_adapter/checkpoint-216000/model.safetensors\"\n",
    "    data_json_file = \"/home/researcher/Documents/dataset/original_datasets/webui_prompts.json\"\n",
    "    resolution = 256\n",
    "    dataloader_num_workers = 8\n",
    "    dataset_name = \"everything_else\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ARGS' object has no attribute 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     14\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./configs/Tracking.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m), Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[0;32m---> 15\u001b[0m     Path(\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     yaml\u001b[38;5;241m.\u001b[39mdump(config, \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     17\u001b[0m     eyeFormer \u001b[38;5;241m=\u001b[39m TrackingTransformer(config \u001b[38;5;241m=\u001b[39m config, init_deit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ARGS' object has no attribute 'output_dir'"
     ]
    }
   ],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "\n",
    "flowAE = FlowEncoder_MLP()\n",
    "flow_latenizer = flowAE.encoder\n",
    "\n",
    "if args.dataset_name == \"ueyes\":\n",
    "    flowAE.load_state_dict(torch.load(\"/home/researcher/flowAE.pth\")) # TODO: Put the right path here\n",
    "    eyeFormer = None\n",
    "else: \n",
    "    config = yaml.load(open(\"./configs/Tracking.yaml\", 'r'), Loader=yaml.Loader)\n",
    "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))\n",
    "    eyeFormer = TrackingTransformer(config = config, init_deit=False)\n",
    "    checkpointEF = torch.load(\"/home/researcher/Documents/aryan/asciProject/flowEncoder/weights/checkpoint_19.pth\",\n",
    "                            map_location='cpu')\n",
    "    state_dict = checkpointEF['model']\n",
    "    eyeFormer.load_state_dict(state_dict)\n",
    "    eyeFormer.requires_grad_(False)\n",
    "    \n",
    "flow_latenizer.requires_grad_(False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in /home/researcher/anaconda3/envs/control/lib/python3.8/site-packages (0.4.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install safetensors\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_proj_model.norm.bias\n",
      "correct_proj_model.norm.weight\n",
      "correct_proj_model.proj.bias\n",
      "correct_proj_model.proj.weight\n",
      "unet.conv_in.bias\n",
      "unet.conv_in.weight\n",
      "unet.conv_norm_out.bias\n",
      "unet.conv_norm_out.weight\n",
      "unet.conv_out.bias\n",
      "unet.conv_out.weight\n",
      "unet.down_blocks.0.attentions.0.norm.bias\n",
      "unet.down_blocks.0.attentions.0.norm.weight\n",
      "unet.down_blocks.0.attentions.0.proj_in.bias\n",
      "unet.down_blocks.0.attentions.0.proj_in.weight\n",
      "unet.down_blocks.0.attentions.0.proj_out.bias\n",
      "unet.down_blocks.0.attentions.0.proj_out.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.0.attentions.1.norm.bias\n",
      "unet.down_blocks.0.attentions.1.norm.weight\n",
      "unet.down_blocks.0.attentions.1.proj_in.bias\n",
      "unet.down_blocks.0.attentions.1.proj_in.weight\n",
      "unet.down_blocks.0.attentions.1.proj_out.bias\n",
      "unet.down_blocks.0.attentions.1.proj_out.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.0.downsamplers.0.conv.bias\n",
      "unet.down_blocks.0.downsamplers.0.conv.weight\n",
      "unet.down_blocks.0.resnets.0.conv1.bias\n",
      "unet.down_blocks.0.resnets.0.conv1.weight\n",
      "unet.down_blocks.0.resnets.0.conv2.bias\n",
      "unet.down_blocks.0.resnets.0.conv2.weight\n",
      "unet.down_blocks.0.resnets.0.norm1.bias\n",
      "unet.down_blocks.0.resnets.0.norm1.weight\n",
      "unet.down_blocks.0.resnets.0.norm2.bias\n",
      "unet.down_blocks.0.resnets.0.norm2.weight\n",
      "unet.down_blocks.0.resnets.0.time_emb_proj.bias\n",
      "unet.down_blocks.0.resnets.0.time_emb_proj.weight\n",
      "unet.down_blocks.0.resnets.1.conv1.bias\n",
      "unet.down_blocks.0.resnets.1.conv1.weight\n",
      "unet.down_blocks.0.resnets.1.conv2.bias\n",
      "unet.down_blocks.0.resnets.1.conv2.weight\n",
      "unet.down_blocks.0.resnets.1.norm1.bias\n",
      "unet.down_blocks.0.resnets.1.norm1.weight\n",
      "unet.down_blocks.0.resnets.1.norm2.bias\n",
      "unet.down_blocks.0.resnets.1.norm2.weight\n",
      "unet.down_blocks.0.resnets.1.time_emb_proj.bias\n",
      "unet.down_blocks.0.resnets.1.time_emb_proj.weight\n",
      "unet.down_blocks.1.attentions.0.norm.bias\n",
      "unet.down_blocks.1.attentions.0.norm.weight\n",
      "unet.down_blocks.1.attentions.0.proj_in.bias\n",
      "unet.down_blocks.1.attentions.0.proj_in.weight\n",
      "unet.down_blocks.1.attentions.0.proj_out.bias\n",
      "unet.down_blocks.1.attentions.0.proj_out.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.1.attentions.1.norm.bias\n",
      "unet.down_blocks.1.attentions.1.norm.weight\n",
      "unet.down_blocks.1.attentions.1.proj_in.bias\n",
      "unet.down_blocks.1.attentions.1.proj_in.weight\n",
      "unet.down_blocks.1.attentions.1.proj_out.bias\n",
      "unet.down_blocks.1.attentions.1.proj_out.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.1.downsamplers.0.conv.bias\n",
      "unet.down_blocks.1.downsamplers.0.conv.weight\n",
      "unet.down_blocks.1.resnets.0.conv1.bias\n",
      "unet.down_blocks.1.resnets.0.conv1.weight\n",
      "unet.down_blocks.1.resnets.0.conv2.bias\n",
      "unet.down_blocks.1.resnets.0.conv2.weight\n",
      "unet.down_blocks.1.resnets.0.conv_shortcut.bias\n",
      "unet.down_blocks.1.resnets.0.conv_shortcut.weight\n",
      "unet.down_blocks.1.resnets.0.norm1.bias\n",
      "unet.down_blocks.1.resnets.0.norm1.weight\n",
      "unet.down_blocks.1.resnets.0.norm2.bias\n",
      "unet.down_blocks.1.resnets.0.norm2.weight\n",
      "unet.down_blocks.1.resnets.0.time_emb_proj.bias\n",
      "unet.down_blocks.1.resnets.0.time_emb_proj.weight\n",
      "unet.down_blocks.1.resnets.1.conv1.bias\n",
      "unet.down_blocks.1.resnets.1.conv1.weight\n",
      "unet.down_blocks.1.resnets.1.conv2.bias\n",
      "unet.down_blocks.1.resnets.1.conv2.weight\n",
      "unet.down_blocks.1.resnets.1.norm1.bias\n",
      "unet.down_blocks.1.resnets.1.norm1.weight\n",
      "unet.down_blocks.1.resnets.1.norm2.bias\n",
      "unet.down_blocks.1.resnets.1.norm2.weight\n",
      "unet.down_blocks.1.resnets.1.time_emb_proj.bias\n",
      "unet.down_blocks.1.resnets.1.time_emb_proj.weight\n",
      "unet.down_blocks.2.attentions.0.norm.bias\n",
      "unet.down_blocks.2.attentions.0.norm.weight\n",
      "unet.down_blocks.2.attentions.0.proj_in.bias\n",
      "unet.down_blocks.2.attentions.0.proj_in.weight\n",
      "unet.down_blocks.2.attentions.0.proj_out.bias\n",
      "unet.down_blocks.2.attentions.0.proj_out.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.2.attentions.1.norm.bias\n",
      "unet.down_blocks.2.attentions.1.norm.weight\n",
      "unet.down_blocks.2.attentions.1.proj_in.bias\n",
      "unet.down_blocks.2.attentions.1.proj_in.weight\n",
      "unet.down_blocks.2.attentions.1.proj_out.bias\n",
      "unet.down_blocks.2.attentions.1.proj_out.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.down_blocks.2.downsamplers.0.conv.bias\n",
      "unet.down_blocks.2.downsamplers.0.conv.weight\n",
      "unet.down_blocks.2.resnets.0.conv1.bias\n",
      "unet.down_blocks.2.resnets.0.conv1.weight\n",
      "unet.down_blocks.2.resnets.0.conv2.bias\n",
      "unet.down_blocks.2.resnets.0.conv2.weight\n",
      "unet.down_blocks.2.resnets.0.conv_shortcut.bias\n",
      "unet.down_blocks.2.resnets.0.conv_shortcut.weight\n",
      "unet.down_blocks.2.resnets.0.norm1.bias\n",
      "unet.down_blocks.2.resnets.0.norm1.weight\n",
      "unet.down_blocks.2.resnets.0.norm2.bias\n",
      "unet.down_blocks.2.resnets.0.norm2.weight\n",
      "unet.down_blocks.2.resnets.0.time_emb_proj.bias\n",
      "unet.down_blocks.2.resnets.0.time_emb_proj.weight\n",
      "unet.down_blocks.2.resnets.1.conv1.bias\n",
      "unet.down_blocks.2.resnets.1.conv1.weight\n",
      "unet.down_blocks.2.resnets.1.conv2.bias\n",
      "unet.down_blocks.2.resnets.1.conv2.weight\n",
      "unet.down_blocks.2.resnets.1.norm1.bias\n",
      "unet.down_blocks.2.resnets.1.norm1.weight\n",
      "unet.down_blocks.2.resnets.1.norm2.bias\n",
      "unet.down_blocks.2.resnets.1.norm2.weight\n",
      "unet.down_blocks.2.resnets.1.time_emb_proj.bias\n",
      "unet.down_blocks.2.resnets.1.time_emb_proj.weight\n",
      "unet.down_blocks.3.resnets.0.conv1.bias\n",
      "unet.down_blocks.3.resnets.0.conv1.weight\n",
      "unet.down_blocks.3.resnets.0.conv2.bias\n",
      "unet.down_blocks.3.resnets.0.conv2.weight\n",
      "unet.down_blocks.3.resnets.0.norm1.bias\n",
      "unet.down_blocks.3.resnets.0.norm1.weight\n",
      "unet.down_blocks.3.resnets.0.norm2.bias\n",
      "unet.down_blocks.3.resnets.0.norm2.weight\n",
      "unet.down_blocks.3.resnets.0.time_emb_proj.bias\n",
      "unet.down_blocks.3.resnets.0.time_emb_proj.weight\n",
      "unet.down_blocks.3.resnets.1.conv1.bias\n",
      "unet.down_blocks.3.resnets.1.conv1.weight\n",
      "unet.down_blocks.3.resnets.1.conv2.bias\n",
      "unet.down_blocks.3.resnets.1.conv2.weight\n",
      "unet.down_blocks.3.resnets.1.norm1.bias\n",
      "unet.down_blocks.3.resnets.1.norm1.weight\n",
      "unet.down_blocks.3.resnets.1.norm2.bias\n",
      "unet.down_blocks.3.resnets.1.norm2.weight\n",
      "unet.down_blocks.3.resnets.1.time_emb_proj.bias\n",
      "unet.down_blocks.3.resnets.1.time_emb_proj.weight\n",
      "unet.mid_block.attentions.0.norm.bias\n",
      "unet.mid_block.attentions.0.norm.weight\n",
      "unet.mid_block.attentions.0.proj_in.bias\n",
      "unet.mid_block.attentions.0.proj_in.weight\n",
      "unet.mid_block.attentions.0.proj_out.bias\n",
      "unet.mid_block.attentions.0.proj_out.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.mid_block.resnets.0.conv1.bias\n",
      "unet.mid_block.resnets.0.conv1.weight\n",
      "unet.mid_block.resnets.0.conv2.bias\n",
      "unet.mid_block.resnets.0.conv2.weight\n",
      "unet.mid_block.resnets.0.norm1.bias\n",
      "unet.mid_block.resnets.0.norm1.weight\n",
      "unet.mid_block.resnets.0.norm2.bias\n",
      "unet.mid_block.resnets.0.norm2.weight\n",
      "unet.mid_block.resnets.0.time_emb_proj.bias\n",
      "unet.mid_block.resnets.0.time_emb_proj.weight\n",
      "unet.mid_block.resnets.1.conv1.bias\n",
      "unet.mid_block.resnets.1.conv1.weight\n",
      "unet.mid_block.resnets.1.conv2.bias\n",
      "unet.mid_block.resnets.1.conv2.weight\n",
      "unet.mid_block.resnets.1.norm1.bias\n",
      "unet.mid_block.resnets.1.norm1.weight\n",
      "unet.mid_block.resnets.1.norm2.bias\n",
      "unet.mid_block.resnets.1.norm2.weight\n",
      "unet.mid_block.resnets.1.time_emb_proj.bias\n",
      "unet.mid_block.resnets.1.time_emb_proj.weight\n",
      "unet.time_embedding.linear_1.bias\n",
      "unet.time_embedding.linear_1.weight\n",
      "unet.time_embedding.linear_2.bias\n",
      "unet.time_embedding.linear_2.weight\n",
      "unet.up_blocks.0.resnets.0.conv1.bias\n",
      "unet.up_blocks.0.resnets.0.conv1.weight\n",
      "unet.up_blocks.0.resnets.0.conv2.bias\n",
      "unet.up_blocks.0.resnets.0.conv2.weight\n",
      "unet.up_blocks.0.resnets.0.conv_shortcut.bias\n",
      "unet.up_blocks.0.resnets.0.conv_shortcut.weight\n",
      "unet.up_blocks.0.resnets.0.norm1.bias\n",
      "unet.up_blocks.0.resnets.0.norm1.weight\n",
      "unet.up_blocks.0.resnets.0.norm2.bias\n",
      "unet.up_blocks.0.resnets.0.norm2.weight\n",
      "unet.up_blocks.0.resnets.0.time_emb_proj.bias\n",
      "unet.up_blocks.0.resnets.0.time_emb_proj.weight\n",
      "unet.up_blocks.0.resnets.1.conv1.bias\n",
      "unet.up_blocks.0.resnets.1.conv1.weight\n",
      "unet.up_blocks.0.resnets.1.conv2.bias\n",
      "unet.up_blocks.0.resnets.1.conv2.weight\n",
      "unet.up_blocks.0.resnets.1.conv_shortcut.bias\n",
      "unet.up_blocks.0.resnets.1.conv_shortcut.weight\n",
      "unet.up_blocks.0.resnets.1.norm1.bias\n",
      "unet.up_blocks.0.resnets.1.norm1.weight\n",
      "unet.up_blocks.0.resnets.1.norm2.bias\n",
      "unet.up_blocks.0.resnets.1.norm2.weight\n",
      "unet.up_blocks.0.resnets.1.time_emb_proj.bias\n",
      "unet.up_blocks.0.resnets.1.time_emb_proj.weight\n",
      "unet.up_blocks.0.resnets.2.conv1.bias\n",
      "unet.up_blocks.0.resnets.2.conv1.weight\n",
      "unet.up_blocks.0.resnets.2.conv2.bias\n",
      "unet.up_blocks.0.resnets.2.conv2.weight\n",
      "unet.up_blocks.0.resnets.2.conv_shortcut.bias\n",
      "unet.up_blocks.0.resnets.2.conv_shortcut.weight\n",
      "unet.up_blocks.0.resnets.2.norm1.bias\n",
      "unet.up_blocks.0.resnets.2.norm1.weight\n",
      "unet.up_blocks.0.resnets.2.norm2.bias\n",
      "unet.up_blocks.0.resnets.2.norm2.weight\n",
      "unet.up_blocks.0.resnets.2.time_emb_proj.bias\n",
      "unet.up_blocks.0.resnets.2.time_emb_proj.weight\n",
      "unet.up_blocks.0.upsamplers.0.conv.bias\n",
      "unet.up_blocks.0.upsamplers.0.conv.weight\n",
      "unet.up_blocks.1.attentions.0.norm.bias\n",
      "unet.up_blocks.1.attentions.0.norm.weight\n",
      "unet.up_blocks.1.attentions.0.proj_in.bias\n",
      "unet.up_blocks.1.attentions.0.proj_in.weight\n",
      "unet.up_blocks.1.attentions.0.proj_out.bias\n",
      "unet.up_blocks.1.attentions.0.proj_out.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.1.attentions.1.norm.bias\n",
      "unet.up_blocks.1.attentions.1.norm.weight\n",
      "unet.up_blocks.1.attentions.1.proj_in.bias\n",
      "unet.up_blocks.1.attentions.1.proj_in.weight\n",
      "unet.up_blocks.1.attentions.1.proj_out.bias\n",
      "unet.up_blocks.1.attentions.1.proj_out.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.1.attentions.2.norm.bias\n",
      "unet.up_blocks.1.attentions.2.norm.weight\n",
      "unet.up_blocks.1.attentions.2.proj_in.bias\n",
      "unet.up_blocks.1.attentions.2.proj_in.weight\n",
      "unet.up_blocks.1.attentions.2.proj_out.bias\n",
      "unet.up_blocks.1.attentions.2.proj_out.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.1.resnets.0.conv1.bias\n",
      "unet.up_blocks.1.resnets.0.conv1.weight\n",
      "unet.up_blocks.1.resnets.0.conv2.bias\n",
      "unet.up_blocks.1.resnets.0.conv2.weight\n",
      "unet.up_blocks.1.resnets.0.conv_shortcut.bias\n",
      "unet.up_blocks.1.resnets.0.conv_shortcut.weight\n",
      "unet.up_blocks.1.resnets.0.norm1.bias\n",
      "unet.up_blocks.1.resnets.0.norm1.weight\n",
      "unet.up_blocks.1.resnets.0.norm2.bias\n",
      "unet.up_blocks.1.resnets.0.norm2.weight\n",
      "unet.up_blocks.1.resnets.0.time_emb_proj.bias\n",
      "unet.up_blocks.1.resnets.0.time_emb_proj.weight\n",
      "unet.up_blocks.1.resnets.1.conv1.bias\n",
      "unet.up_blocks.1.resnets.1.conv1.weight\n",
      "unet.up_blocks.1.resnets.1.conv2.bias\n",
      "unet.up_blocks.1.resnets.1.conv2.weight\n",
      "unet.up_blocks.1.resnets.1.conv_shortcut.bias\n",
      "unet.up_blocks.1.resnets.1.conv_shortcut.weight\n",
      "unet.up_blocks.1.resnets.1.norm1.bias\n",
      "unet.up_blocks.1.resnets.1.norm1.weight\n",
      "unet.up_blocks.1.resnets.1.norm2.bias\n",
      "unet.up_blocks.1.resnets.1.norm2.weight\n",
      "unet.up_blocks.1.resnets.1.time_emb_proj.bias\n",
      "unet.up_blocks.1.resnets.1.time_emb_proj.weight\n",
      "unet.up_blocks.1.resnets.2.conv1.bias\n",
      "unet.up_blocks.1.resnets.2.conv1.weight\n",
      "unet.up_blocks.1.resnets.2.conv2.bias\n",
      "unet.up_blocks.1.resnets.2.conv2.weight\n",
      "unet.up_blocks.1.resnets.2.conv_shortcut.bias\n",
      "unet.up_blocks.1.resnets.2.conv_shortcut.weight\n",
      "unet.up_blocks.1.resnets.2.norm1.bias\n",
      "unet.up_blocks.1.resnets.2.norm1.weight\n",
      "unet.up_blocks.1.resnets.2.norm2.bias\n",
      "unet.up_blocks.1.resnets.2.norm2.weight\n",
      "unet.up_blocks.1.resnets.2.time_emb_proj.bias\n",
      "unet.up_blocks.1.resnets.2.time_emb_proj.weight\n",
      "unet.up_blocks.1.upsamplers.0.conv.bias\n",
      "unet.up_blocks.1.upsamplers.0.conv.weight\n",
      "unet.up_blocks.2.attentions.0.norm.bias\n",
      "unet.up_blocks.2.attentions.0.norm.weight\n",
      "unet.up_blocks.2.attentions.0.proj_in.bias\n",
      "unet.up_blocks.2.attentions.0.proj_in.weight\n",
      "unet.up_blocks.2.attentions.0.proj_out.bias\n",
      "unet.up_blocks.2.attentions.0.proj_out.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.2.attentions.1.norm.bias\n",
      "unet.up_blocks.2.attentions.1.norm.weight\n",
      "unet.up_blocks.2.attentions.1.proj_in.bias\n",
      "unet.up_blocks.2.attentions.1.proj_in.weight\n",
      "unet.up_blocks.2.attentions.1.proj_out.bias\n",
      "unet.up_blocks.2.attentions.1.proj_out.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.2.attentions.2.norm.bias\n",
      "unet.up_blocks.2.attentions.2.norm.weight\n",
      "unet.up_blocks.2.attentions.2.proj_in.bias\n",
      "unet.up_blocks.2.attentions.2.proj_in.weight\n",
      "unet.up_blocks.2.attentions.2.proj_out.bias\n",
      "unet.up_blocks.2.attentions.2.proj_out.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.2.resnets.0.conv1.bias\n",
      "unet.up_blocks.2.resnets.0.conv1.weight\n",
      "unet.up_blocks.2.resnets.0.conv2.bias\n",
      "unet.up_blocks.2.resnets.0.conv2.weight\n",
      "unet.up_blocks.2.resnets.0.conv_shortcut.bias\n",
      "unet.up_blocks.2.resnets.0.conv_shortcut.weight\n",
      "unet.up_blocks.2.resnets.0.norm1.bias\n",
      "unet.up_blocks.2.resnets.0.norm1.weight\n",
      "unet.up_blocks.2.resnets.0.norm2.bias\n",
      "unet.up_blocks.2.resnets.0.norm2.weight\n",
      "unet.up_blocks.2.resnets.0.time_emb_proj.bias\n",
      "unet.up_blocks.2.resnets.0.time_emb_proj.weight\n",
      "unet.up_blocks.2.resnets.1.conv1.bias\n",
      "unet.up_blocks.2.resnets.1.conv1.weight\n",
      "unet.up_blocks.2.resnets.1.conv2.bias\n",
      "unet.up_blocks.2.resnets.1.conv2.weight\n",
      "unet.up_blocks.2.resnets.1.conv_shortcut.bias\n",
      "unet.up_blocks.2.resnets.1.conv_shortcut.weight\n",
      "unet.up_blocks.2.resnets.1.norm1.bias\n",
      "unet.up_blocks.2.resnets.1.norm1.weight\n",
      "unet.up_blocks.2.resnets.1.norm2.bias\n",
      "unet.up_blocks.2.resnets.1.norm2.weight\n",
      "unet.up_blocks.2.resnets.1.time_emb_proj.bias\n",
      "unet.up_blocks.2.resnets.1.time_emb_proj.weight\n",
      "unet.up_blocks.2.resnets.2.conv1.bias\n",
      "unet.up_blocks.2.resnets.2.conv1.weight\n",
      "unet.up_blocks.2.resnets.2.conv2.bias\n",
      "unet.up_blocks.2.resnets.2.conv2.weight\n",
      "unet.up_blocks.2.resnets.2.conv_shortcut.bias\n",
      "unet.up_blocks.2.resnets.2.conv_shortcut.weight\n",
      "unet.up_blocks.2.resnets.2.norm1.bias\n",
      "unet.up_blocks.2.resnets.2.norm1.weight\n",
      "unet.up_blocks.2.resnets.2.norm2.bias\n",
      "unet.up_blocks.2.resnets.2.norm2.weight\n",
      "unet.up_blocks.2.resnets.2.time_emb_proj.bias\n",
      "unet.up_blocks.2.resnets.2.time_emb_proj.weight\n",
      "unet.up_blocks.2.upsamplers.0.conv.bias\n",
      "unet.up_blocks.2.upsamplers.0.conv.weight\n",
      "unet.up_blocks.3.attentions.0.norm.bias\n",
      "unet.up_blocks.3.attentions.0.norm.weight\n",
      "unet.up_blocks.3.attentions.0.proj_in.bias\n",
      "unet.up_blocks.3.attentions.0.proj_in.weight\n",
      "unet.up_blocks.3.attentions.0.proj_out.bias\n",
      "unet.up_blocks.3.attentions.0.proj_out.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.3.attentions.1.norm.bias\n",
      "unet.up_blocks.3.attentions.1.norm.weight\n",
      "unet.up_blocks.3.attentions.1.proj_in.bias\n",
      "unet.up_blocks.3.attentions.1.proj_in.weight\n",
      "unet.up_blocks.3.attentions.1.proj_out.bias\n",
      "unet.up_blocks.3.attentions.1.proj_out.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.3.attentions.2.norm.bias\n",
      "unet.up_blocks.3.attentions.2.norm.weight\n",
      "unet.up_blocks.3.attentions.2.proj_in.bias\n",
      "unet.up_blocks.3.attentions.2.proj_in.weight\n",
      "unet.up_blocks.3.attentions.2.proj_out.bias\n",
      "unet.up_blocks.3.attentions.2.proj_out.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_ip.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_ip.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight\n",
      "unet.up_blocks.3.resnets.0.conv1.bias\n",
      "unet.up_blocks.3.resnets.0.conv1.weight\n",
      "unet.up_blocks.3.resnets.0.conv2.bias\n",
      "unet.up_blocks.3.resnets.0.conv2.weight\n",
      "unet.up_blocks.3.resnets.0.conv_shortcut.bias\n",
      "unet.up_blocks.3.resnets.0.conv_shortcut.weight\n",
      "unet.up_blocks.3.resnets.0.norm1.bias\n",
      "unet.up_blocks.3.resnets.0.norm1.weight\n",
      "unet.up_blocks.3.resnets.0.norm2.bias\n",
      "unet.up_blocks.3.resnets.0.norm2.weight\n",
      "unet.up_blocks.3.resnets.0.time_emb_proj.bias\n",
      "unet.up_blocks.3.resnets.0.time_emb_proj.weight\n",
      "unet.up_blocks.3.resnets.1.conv1.bias\n",
      "unet.up_blocks.3.resnets.1.conv1.weight\n",
      "unet.up_blocks.3.resnets.1.conv2.bias\n",
      "unet.up_blocks.3.resnets.1.conv2.weight\n",
      "unet.up_blocks.3.resnets.1.conv_shortcut.bias\n",
      "unet.up_blocks.3.resnets.1.conv_shortcut.weight\n",
      "unet.up_blocks.3.resnets.1.norm1.bias\n",
      "unet.up_blocks.3.resnets.1.norm1.weight\n",
      "unet.up_blocks.3.resnets.1.norm2.bias\n",
      "unet.up_blocks.3.resnets.1.norm2.weight\n",
      "unet.up_blocks.3.resnets.1.time_emb_proj.bias\n",
      "unet.up_blocks.3.resnets.1.time_emb_proj.weight\n",
      "unet.up_blocks.3.resnets.2.conv1.bias\n",
      "unet.up_blocks.3.resnets.2.conv1.weight\n",
      "unet.up_blocks.3.resnets.2.conv2.bias\n",
      "unet.up_blocks.3.resnets.2.conv2.weight\n",
      "unet.up_blocks.3.resnets.2.conv_shortcut.bias\n",
      "unet.up_blocks.3.resnets.2.conv_shortcut.weight\n",
      "unet.up_blocks.3.resnets.2.norm1.bias\n",
      "unet.up_blocks.3.resnets.2.norm1.weight\n",
      "unet.up_blocks.3.resnets.2.norm2.bias\n",
      "unet.up_blocks.3.resnets.2.norm2.weight\n",
      "unet.up_blocks.3.resnets.2.time_emb_proj.bias\n",
      "unet.up_blocks.3.resnets.2.time_emb_proj.weight\n"
     ]
    }
   ],
   "source": [
    "with safe_open(args.pretrained_ip_adapter_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    for key in f.keys():\n",
    "        if \"correct_proj_model\" in key:\n",
    "            print(key)\n",
    "        elif \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
